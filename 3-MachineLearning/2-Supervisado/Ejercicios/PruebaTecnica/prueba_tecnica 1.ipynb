{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimado candidato,\n",
    "\n",
    "Estamos encantados de que estés considerando unirte a nuestro equipo de ciencia de datos. Como parte de nuestro proceso de evaluación, te pedimos que completes la siguiente prueba técnica.\n",
    "\n",
    "**Descripción de la tarea:**\n",
    "\n",
    "Se te proporciona un conjunto de datos anónimos que consta de varias columnas nombradas como 'col1', 'col2', 'col3', etc., hasta 'col20', y una columna 'target' que representa la variable objetivo. Tu tarea es realizar un análisis exploratorio de datos (EDA) y construir un modelo de aprendizaje automático para predecir la variable 'target'.\n",
    "\n",
    "**Detalles de la tarea:**\n",
    "\n",
    "1. **Análisis exploratorio de datos (EDA):** Realiza un análisis exploratorio detallado de los datos. Esto debe incluir, pero no está limitado a:\n",
    "   - Estadísticas descriptivas de las variables (mínimo, máximo, media, mediana, desviación estándar, etc.).\n",
    "   - Verificación de valores perdidos o anómalos.\n",
    "   - Análisis de correlación entre las variables.\n",
    "   - Visualizaciones para entender mejor las distribuciones y relaciones de los datos.\n",
    "\n",
    "2. **Preprocesamiento de datos:** Prepárate para realizar cualquier limpieza o transformación necesaria de los datos basándote en tu EDA.\n",
    "\n",
    "3. **Creación de un modelo de línea base (Baseline):** Entrena varios modelos de aprendizaje automático para predecir la variable 'target'. Comienza con modelos sencillos como la regresión logística y avanza hacia modelos más complejos como los árboles de decisión, random forest, SVM, XGBoost, etc.\n",
    "\n",
    "4. **Evaluación del modelo:** Evalúa el rendimiento de cada modelo utilizando métricas apropiadas. Por ejemplo, si 'target' es una variable binaria, podrías considerar la precisión, la sensibilidad, la especificidad, el AUC-ROC, etc.\n",
    "\n",
    "5. **Selección del modelo:** Selecciona el modelo que creas que funciona mejor. Justifica tu elección basándote en las métricas de evaluación y cualquier otra consideración pertinente.\n",
    "\n",
    "6. **Predicción:** Usa tu modelo seleccionado para hacer predicciones en el conjunto de datos.\n",
    "\n",
    "**Entregables:**\n",
    "\n",
    "Por favor, proporciona el código que hayas utilizado para completar esta tarea, junto con un informe detallado que explique tu enfoque y los resultados. El informe debe estar bien estructurado y ser fácil de seguir, permitiendo a los evaluadores entender tu proceso de pensamiento y las decisiones que hayas tomado.\n",
    "\n",
    "¡Buena suerte y esperamos ver tu solución!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera Investigación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas descriptivas de las variables (mínimo, máximo, media, mediana, desviación estándar, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns!= \"ID\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de valores perdidos o anómalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df[\"col1\"])\n",
    "plt.title(\"Boxplot de ID\")\n",
    "plt.ylabel(\"Valores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de correlación entre las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOT THIS AS A GUIDE FROM CHATGPT\n",
    "\n",
    "Cuando te encuentras con un conjunto de datos en el que las columnas tienen nombres poco descriptivos (por ejemplo, “col1”, “col2”, etc.) y todos los datos son numéricos, el EDA (análisis exploratorio de datos) se centra en descubrir patrones, relaciones y características estadísticas de cada variable, sin depender de la semántica de los nombres. Aquí te dejo un enfoque paso a paso:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Resumen Estadístico Inicial\n",
    "\n",
    "- **Estadísticas descriptivas:**  \n",
    "  Usa métodos como `df.describe()` para obtener la media, mediana, desviación estándar, percentiles, mínimos y máximos. Esto te dará una primera idea de la escala, dispersión y posibles valores extremos.\n",
    "\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  \n",
    "  # Supongamos que ya tienes cargado el DataFrame en df\n",
    "  resumen = df.describe()\n",
    "  print(resumen)\n",
    "  ```\n",
    "\n",
    "- **Valores faltantes:**  \n",
    "  Verifica si existen datos ausentes con `df.isnull().sum()`. Esto te ayudará a saber si necesitas imputar o limpiar datos.\n",
    "\n",
    "  ```python\n",
    "  print(df.isnull().sum())\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Visualización de Distribuciones\n",
    "\n",
    "Dado que los nombres no aportan contexto, es importante conocer cómo se distribuyen los valores de cada columna:\n",
    "\n",
    "- **Histogramas y curvas de densidad:**  \n",
    "  Permiten ver la forma de la distribución de cada variable (si son simétricas, sesgadas, si hay múltiples picos, etc.).\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  \n",
    "  for col in df.columns:\n",
    "      plt.figure(figsize=(6, 4))\n",
    "      sns.histplot(df[col], kde=True, bins=30)\n",
    "      plt.title(f'Distribución de {col}')\n",
    "      plt.xlabel(col)\n",
    "      plt.ylabel('Frecuencia')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "- **Boxplots:**  \n",
    "  Son útiles para detectar valores atípicos y ver la dispersión de los datos.\n",
    "\n",
    "  ```python\n",
    "  for col in df.columns:\n",
    "      plt.figure(figsize=(6, 2))\n",
    "      sns.boxplot(x=df[col])\n",
    "      plt.title(f'Boxplot de {col}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Análisis de Relaciones\n",
    "\n",
    "Sin nombres informativos, tendrás que inferir relaciones entre variables a partir de sus comportamientos numéricos:\n",
    "\n",
    "- **Matriz de correlación:**  \n",
    "  Calcula la correlación entre todas las variables para identificar pares de variables que se mueven de forma similar o inversa.\n",
    "\n",
    "  ```python\n",
    "  corr = df.corr()\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "  plt.title(\"Mapa de calor de la matriz de correlación\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Pairplots:**  \n",
    "  Un `pairplot` de Seaborn te permite ver gráficos de dispersión para cada par de variables junto con sus distribuciones univariadas. Esto puede revelar agrupaciones o relaciones no lineales.\n",
    "\n",
    "  ```python\n",
    "  sns.pairplot(df)\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reducción de Dimensionalidad\n",
    "\n",
    "Si el número de columnas es elevado y la semántica es poco clara, técnicas como el Análisis de Componentes Principales (PCA) pueden ayudarte a descubrir patrones subyacentes:\n",
    "\n",
    "- **PCA para detectar estructuras latentes:**\n",
    "\n",
    "  ```python\n",
    "  from sklearn.decomposition import PCA\n",
    "  import numpy as np\n",
    "  \n",
    "  # Estandariza los datos (muy recomendado para PCA)\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  df_scaled = scaler.fit_transform(df)\n",
    "  \n",
    "  pca = PCA(n_components=2)  # reduce a 2 dimensiones para visualización\n",
    "  componentes = pca.fit_transform(df_scaled)\n",
    "  \n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.scatter(componentes[:, 0], componentes[:, 1], alpha=0.7)\n",
    "  plt.xlabel(\"Componente 1\")\n",
    "  plt.ylabel(\"Componente 2\")\n",
    "  plt.title(\"Proyección PCA de los datos\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "Esta técnica te ayuda a visualizar la estructura global del conjunto de datos y puede sugerir agrupamientos o patrones que no son evidentes en el espacio original.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Identificación de Patrones y Agrupaciones\n",
    "\n",
    "- **Clustering:**  \n",
    "  Algoritmos de clustering (por ejemplo, K-means) pueden ayudarte a identificar grupos o segmentos dentro de los datos, incluso cuando los nombres de las columnas no son informativos.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.cluster import KMeans\n",
    "  \n",
    "  kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "  clusters = kmeans.fit_predict(df_scaled)\n",
    "  \n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.scatter(componentes[:, 0], componentes[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "  plt.xlabel(\"Componente 1\")\n",
    "  plt.ylabel(\"Componente 2\")\n",
    "  plt.title(\"Clustering en el espacio PCA\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Cuando los nombres de las columnas no aportan información y solo cuentas con datos numéricos, el EDA se basa en técnicas estadísticas y visualizaciones para descubrir la estructura y las relaciones entre variables. Siguiendo estos pasos:\n",
    "- Resumen estadístico y verificación de valores perdidos.\n",
    "- Visualización de distribuciones (histogramas, boxplots, etc.).\n",
    "- Análisis de correlaciones y relaciones (matriz de correlación, pairplots).\n",
    "- Aplicación de técnicas de reducción de dimensionalidad (PCA) y clustering.\n",
    "\n",
    "Esta metodología te permitirá explorar a fondo el comportamiento de tus datos y descubrir patrones o anomalías que sirvan de guía para un análisis posterior o para la toma de decisiones.\n",
    "\n",
    "Cada uno de estos pasos te ayudará a construir una imagen completa del conjunto de datos, a pesar de la falta de nombres descriptivos, basándote en las propiedades numéricas y las relaciones inherentes entre las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaciones para entender mejor las distribuciones y relaciones de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepárate para realizar cualquier limpieza o transformación necesaria de los datos basándote en tu EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando te enfrentas a un conjunto de datos sin información clara sobre qué representa cada columna, el preprocesamiento se vuelve esencial para transformar y limpiar los datos de modo que puedas extraerles información útil. Aquí tienes una estrategia paso a paso:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Exploración Inicial y Comprensión de la Estructura\n",
    "\n",
    "- **Carga y Visualización:**  \n",
    "  Comienza cargando el conjunto de datos (por ejemplo, con pandas) y visualizando las primeras filas para tener una idea general de la estructura.  \n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv(\"tus_datos.csv\")\n",
    "  print(df.head())\n",
    "  ```\n",
    "  \n",
    "- **Revisión de Tipos y Estadísticas Básicas:**  \n",
    "  Usa `df.info()` y `df.describe()` para conocer los tipos de datos (numéricos, categóricos) y obtener un resumen estadístico (media, mediana, percentiles, etc.). Esto te ayudará a identificar rangos, dispersión y valores extremos.\n",
    "  ```python\n",
    "  print(df.info())\n",
    "  print(df.describe())\n",
    "  ```\n",
    "  \n",
    "- **Visualización de Distribuciones:**  \n",
    "  Aunque los nombres no aporten significado, visualizar las distribuciones (con histogramas, boxplots o pairplots) te permitirá ver patrones, simetrías, outliers y rangos de valores.\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "\n",
    "  for col in df.columns:\n",
    "      plt.figure(figsize=(6, 4))\n",
    "      sns.histplot(df[col], bins=30, kde=True)\n",
    "      plt.title(f'Distribución de {col}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Limpieza de Datos\n",
    "\n",
    "- **Valores Faltantes:**  \n",
    "  Identifica y cuenta los valores nulos. Decide si imputar (por ejemplo, con la media, mediana u otra técnica) o eliminar filas/columnas con demasiados datos ausentes.\n",
    "  ```python\n",
    "  print(df.isnull().sum())\n",
    "  df.fillna(df.median(), inplace=True)  # Ejemplo de imputación\n",
    "  ```\n",
    "  \n",
    "- **Detección de Outliers:**  \n",
    "  Sin saber el significado, los outliers pueden distorsionar el análisis. Utiliza métodos como el rango intercuartílico (IQR) o el Z-score para identificarlos y, según el contexto, decide si los eliminas o los transformas.\n",
    "  ```python\n",
    "  Q1 = df.quantile(0.25)\n",
    "  Q3 = df.quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "  df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transformación y Escalado\n",
    "\n",
    "- **Estandarización y Normalización:**  \n",
    "  Dado que no sabes qué representan las columnas, es prudente escalar los datos para que todas tengan la misma unidad o rango. Esto es especialmente útil antes de aplicar técnicas de reducción de dimensionalidad o clustering.\n",
    "  ```python\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "  ```\n",
    "  \n",
    "- **Reducción de Dimensionalidad:**  \n",
    "  Si el número de columnas es elevado o las relaciones entre variables no son claras, técnicas como PCA (Análisis de Componentes Principales) pueden ayudarte a identificar patrones subyacentes y a visualizar la estructura de los datos.\n",
    "  ```python\n",
    "  from sklearn.decomposition import PCA\n",
    "  pca = PCA(n_components=2)\n",
    "  componentes = pca.fit_transform(df_scaled)\n",
    "  \n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.scatter(componentes[:, 0], componentes[:, 1], alpha=0.7)\n",
    "  plt.xlabel(\"Componente 1\")\n",
    "  plt.ylabel(\"Componente 2\")\n",
    "  plt.title(\"Proyección PCA de los datos\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Identificación de Patrones Sin Información Semántica\n",
    "\n",
    "- **Clustering:**  \n",
    "  Aplicar algoritmos de clustering (como K-means) puede ayudarte a descubrir grupos o segmentos en los datos, lo cual es especialmente útil cuando no dispones de etiquetas descriptivas.\n",
    "  ```python\n",
    "  from sklearn.cluster import KMeans\n",
    "  kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "  clusters = kmeans.fit_predict(df_scaled)\n",
    "  \n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.scatter(componentes[:, 0], componentes[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "  plt.xlabel(\"Componente 1\")\n",
    "  plt.ylabel(\"Componente 2\")\n",
    "  plt.title(\"Clustering en el espacio PCA\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Cuando los datos no vienen acompañados de información semántica en los nombres de las columnas, el proceso de EDA se basa en:\n",
    "\n",
    "1. **Exploración y resumen:** Revisar la estructura, tipos y distribuciones con herramientas como `describe()`, `info()`, histogramas y boxplots.\n",
    "2. **Limpieza:** Identificar y tratar valores faltantes y outliers.\n",
    "3. **Transformación:** Escalar y normalizar los datos, y aplicar técnicas de reducción de dimensionalidad si es necesario.\n",
    "4. **Identificación de patrones:** Utilizar clustering y análisis de correlaciones para detectar relaciones entre las variables.\n",
    "\n",
    "Esta estrategia te permitirá transformar un conjunto de datos \"oscuro\" en una fuente de información útil, incluso cuando no tienes una comprensión inmediata de lo que representan las columnas.  \n",
    "\n",
    "Recursos como tutoriales en DataCamp y artículos de visualización con Python (por ejemplo, el blog de Raul E. López Briega) ofrecen ejemplos prácticos que pueden complementar este enfoque [citeturn1search0][citeturn1search7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Modelo de línea base (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena varios modelos de aprendizaje automático para predecir la variable 'target'. Comienza con modelos sencillos como la regresión logística y avanza hacia modelos más complejos como los árboles de decisión, random forest, SVM, XGBoost, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Evaluación del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúa el rendimiento de cada modelo utilizando métricas apropiadas. Por ejemplo, si 'target' es una variable binaria, podrías considerar la precisión, la sensibilidad, la especificidad, el AUC-ROC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Selección del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciona el modelo que creas que funciona mejor. Justifica tu elección basándote en las métricas de evaluación y cualquier otra consideración pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usa tu modelo seleccionado para hacer predicciones en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp-ds-gDITtNOb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
